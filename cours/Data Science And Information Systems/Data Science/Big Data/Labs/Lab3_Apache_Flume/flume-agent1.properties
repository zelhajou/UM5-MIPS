# config for our "upstream" agent
# takes the output from our 'url-generator' python script & pumps it to local port 5555
# the 'agent2' will then read the data from this port & pump it into HDFS

# set up arbitrary names for agent, source, channels, and sink
# this is an extremely simplistic setup only for demo purposes
agent1.sources = agent1-src
agent1.channels = agent1-chan
agent1.sinks = agent1-sink

# set up our source (output of our log-generator script)
agent1.sources.agent1-src.type = exec
agent1.sources.agent1-src.command = tail -F /tmp/log-generator.log
agent1.sources.agent1-src.channels = agent1-chan

# set up our channel (memory, 1000 event buffer)
agent1.channels.agent1-chan.type = memory
agent1.channels.agent1-chan.capacity = 1000

# set up our sink (write avro file to local port 5555)
agent1.sinks.agent1-sink.type = avro
agent1.sinks.agent1-sink.hostname = localhost
agent1.sinks.agent1-sink.port = 5555
agent1.sinks.agent1-sink.batch-size = 1
agent1.sinks.agent1-sink.channel = agent1-chan
