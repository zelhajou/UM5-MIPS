{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deep Dream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\b.mehdi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\b.mehdi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "c:\\users\\b.mehdi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inception_v3.InceptionV3(weights='imagenet',include_top=False)\n",
    "model.training=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramétrage de la configuration de Dream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_settings = {\n",
    "    \"mixed4\": 1.0,\n",
    "    \"mixed5\": 1.5,\n",
    "    \"mixed6\": 2.0,\n",
    "    \"mixed7\": 2.5,\n",
    "}\n",
    "outputs_dict = dict(\n",
    "    [\n",
    "        (layer.name, layer.output)\n",
    "        for layer in [model.get_layer(name) for name in layer_settings.keys()]\n",
    "    ]\n",
    ")\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la perte à maximiser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(input_image):\n",
    "    features = feature_extractor(input_image)\n",
    "    loss = tf.zeros(shape=())\n",
    "    for name in features.keys():\n",
    "        coeff = layer_settings[name]\n",
    "        activation = features[name]\n",
    "        loss += coeff * tf.reduce_mean(tf.square(activation[:, 2:-2, 2:-2, :]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le processus d'ascension du gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_ascent_step(image, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        loss = compute_loss(image)\n",
    "    grads = tape.gradient(loss, image)\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    image += learning_rate * grads\n",
    "    return loss, image\n",
    "\n",
    "\n",
    "def gradient_ascent_loop(image, iterations, learning_rate, max_loss=None):\n",
    "    for i in range(iterations):\n",
    "        loss, image = gradient_ascent_step(image, learning_rate)\n",
    "        if max_loss is not None and loss > max_loss:\n",
    "            break\n",
    "        print(f\"...Valeur de perte à {i}: {loss:.2f}\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 20.\n",
    "num_octave = 3\n",
    "octave_scale = 1.4\n",
    "iterations = 30\n",
    "max_loss = 15.\n",
    "base_image_path = 'h2m.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ascension de gradients sur différentes échelles successives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = keras.utils.load_img(image_path)\n",
    "    img = keras.utils.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    img = img.reshape((img.shape[1], img.shape[2], 3))\n",
    "    img /= 2.0\n",
    "    img += 0.5\n",
    "    img *= 255.\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement de l'image à l'octave 0 de forme: (183, 316)\n",
      "...Valeur de perte à 0: 1.23\n",
      "...Valeur de perte à 1: 1.35\n",
      "...Valeur de perte à 2: 1.62\n",
      "...Valeur de perte à 3: 1.98\n",
      "...Valeur de perte à 4: 2.34\n",
      "...Valeur de perte à 5: 2.80\n",
      "...Valeur de perte à 6: 3.34\n",
      "...Valeur de perte à 7: 3.87\n",
      "...Valeur de perte à 8: 4.60\n",
      "...Valeur de perte à 9: 5.37\n",
      "...Valeur de perte à 10: 6.13\n",
      "...Valeur de perte à 11: 7.16\n",
      "...Valeur de perte à 12: 8.01\n",
      "...Valeur de perte à 13: 9.02\n",
      "...Valeur de perte à 14: 10.04\n",
      "...Valeur de perte à 15: 10.83\n",
      "...Valeur de perte à 16: 11.86\n",
      "...Valeur de perte à 17: 12.97\n",
      "...Valeur de perte à 18: 14.19\n",
      "Traitement de l'image à l'octave 1 de forme: (257, 442)\n",
      "...Valeur de perte à 0: 1.78\n",
      "...Valeur de perte à 1: 3.27\n",
      "...Valeur de perte à 2: 4.78\n",
      "...Valeur de perte à 3: 6.56\n",
      "...Valeur de perte à 4: 8.64\n",
      "...Valeur de perte à 5: 12.58\n",
      "Traitement de l'image à l'octave 2 de forme: (360, 620)\n",
      "...Valeur de perte à 0: 4.66\n",
      "...Valeur de perte à 1: 10.42\n"
     ]
    }
   ],
   "source": [
    "original_img = preprocess_image(base_image_path)\n",
    "original_shape = original_img.shape[1:3]\n",
    "\n",
    "successive_shapes = [original_shape]\n",
    "for i in range(1, num_octave):\n",
    "    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "successive_shapes = successive_shapes[::-1]\n",
    "\n",
    "shrunk_original_img = tf.image.resize(original_img, successive_shapes[0])\n",
    "\n",
    "img = tf.identity(original_img)\n",
    "for i, shape in enumerate(successive_shapes):\n",
    "    print(f\"Traitement de l'image à l'octave {i} de forme: {shape}\")\n",
    "    img = tf.image.resize(img, shape)\n",
    "    img = gradient_ascent_loop(\n",
    "        img, iterations=iterations, learning_rate=step, max_loss=max_loss\n",
    "    )\n",
    "    upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img, shape)\n",
    "    same_size_original = tf.image.resize(original_img, shape)\n",
    "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
    "    img += lost_detail\n",
    "    shrunk_original_img = tf.image.resize(original_img, shape)\n",
    "    keras.utils.save_img(\"h2m_deepdream/dream_at_scale_\" + str(shape) + \".png\", deprocess_image(img.numpy()))\n",
    "keras.utils.save_img(\"h2m_deepdream/dream_final.png\", deprocess_image(img.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Style transfer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import load_img, img_to_array\n",
    "\n",
    "base_image_path = 'h2m.jpg'\n",
    "\n",
    "style_reference_image_path = 'starry_night.jpg'\n",
    "\n",
    "original_width, original_height = load_img(base_image_path).size\n",
    "img_height = 400\n",
    "img_width = round(original_width * img_height / original_height)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import vgg19\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = keras.utils.load_img(\n",
    "        image_path, target_size=(img_height, img_width))\n",
    "    img = keras.utils.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    img = img.reshape((img_height, img_width, 3))\n",
    "    img[:, :, 0] += 103.939\n",
    "    img[:, :, 1] += 116.779\n",
    "    img[:, :, 2] += 123.68\n",
    "    img = img[:, :, ::-1]\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisation du réseau VGG19 pré-entraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "model = vgg19.VGG19(weights='imagenet',include_top=False)\n",
    "\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La perte de contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(target, combination):\n",
    "    return K.sum(K.square(combination - target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La perte de style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La perte de variation totale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    a = K.square(x[:, :img_height - 1, :img_width - 1, :] - x[:, 1:, :img_width - 1, :])\n",
    "    b = K.square(x[:, :img_height - 1, :img_width - 1, :] - x[:, :img_height - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la perte finale que nous allons minimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content_layer = 'block5_conv2'\n",
    "\n",
    "style_layers = ['block1_conv1','block2_conv1',\n",
    "                'block3_conv1','block4_conv1',\n",
    "                'block5_conv1']\n",
    "\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n",
    "\n",
    "@tf.function\n",
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    "    input_tensor = tf.concat(\n",
    "        [base_image, style_reference_image, combination_image], axis=0\n",
    "    )\n",
    "    features = feature_extractor(input_tensor)\n",
    "    loss = tf.zeros(shape=())\n",
    "    layer_features = features[content_layer]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    loss = loss + content_weight * content_loss(\n",
    "        base_image_features, combination_features\n",
    "    )\n",
    "    for layer_name in style_layers:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        style_loss_value = style_loss(style_reference_features, combination_features)\n",
    "        loss += (style_weight / len(style_layers)) * style_loss_value\n",
    "\n",
    "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le processus de descente du gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(combination_image, base_image, style_reference_image)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96))\n",
    "\n",
    "base_image = preprocess_image(base_image_path)\n",
    "style_reference_image = preprocess_image(style_reference_image_path)\n",
    "combination_image = tf.Variable(preprocess_image(base_image_path))\n",
    "\n",
    "iterations = 4000\n",
    "for i in range(1, iterations + 1):\n",
    "    print(i)\n",
    "    loss, grads = compute_loss_and_grads(\n",
    "        combination_image, base_image, style_reference_image\n",
    "    )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i}: loss={loss:.2f}\")\n",
    "        img = deprocess_image(np.array(combination_image))\n",
    "        fname = f\"h2m_style/combination_image_at_iteration_{i}.png\"\n",
    "        keras.utils.save_img(fname, img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
