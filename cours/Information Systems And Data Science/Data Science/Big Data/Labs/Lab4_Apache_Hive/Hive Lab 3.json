{"paragraphs":[{"title":"Hive DML in action","text":"%md\nNow that you have learned how to create databases, tables, and partitions in Hive, you are ready to work with Hive’s Data Manipulation Language and get some work done! Let’s load some data and start writing queries.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194739_628479355","id":"20161116-145016_102302275","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now that you have learned how to create databases, tables, and partitions in Hive, you are ready to work with Hive’s Data Manipulation Language and get some work done! Let’s load some data and start writing queries.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6536"},{"title":"Lab 3 Tasks","text":"%md\nYou are to go through this notebook and follow the instructions in either the markdown paragraphs, or the actual Hive paragraphs. You will run the paragraphs by clicking the “Play” button to the right of each paragraph. Running a markdown paragraph will just print the text out onto the results pane.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194742_627325108","id":"20161116-145051_1959494771","result":{"code":"SUCCESS","type":"HTML","msg":"<p>You are to go through this notebook and follow the instructions in either the markdown paragraphs, or the actual Hive paragraphs. You will run the paragraphs by clicking the “Play” button to the right of each paragraph. Running a markdown paragraph will just print the text out onto the results pane.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6537"},{"title":"Sample Data Descriptions","text":"%md\nFor your convenience, the metadata for the sample data files is listed out here again.\n\nOur sample data is from a fictitious computer retailer. The company sells computer parts and generally serves a single State in the country.\n\n<h3>Customer.csv:</h3>\n\tPurpose: Hold customer records.\n\t\n\tColumns: \n\tFNAME\t                    Customer’s First Name\n\tLNAME\t                    Customer’s Last Name\n\tSTATUS\t                    Active or Inactive status\n\tTELNO\t                    Telephone #\tCustomer’s unique ID\n\tCUSTOMER_ID\tCITY|ZIP        City and Zip code separated by the “|” character.\n\t\n\t\n<h3>Product.csv:</h3>\n\tPurpose: Hold product records.\n\t\n\tColumns:\n\tPROD_NAME\t                Name of product\n\tDESCRIPTION\t                Description of computer product\n\tCATEGORY                    Category product belongs to\t\n\tQTY_ON_HAND\t                Quantity of product in warehouse\n\tPROD_NUM                    Unique product number\t\n\tPACKAGED_WITH               Colon separated list of things that come in package with product.\n\t\t\t\t\t\n\n<h3>Sales.csv:</h3>\n\tPurpose: Holds all historical sales records. Company updates once a month. \n\t\n\tColumns:\n\tCUST_ID\t                    ID of customer who made purchase\n\tPROD_NUM                    ID of product that was purchased\n\tQTY                         QTY purchased\n\tDATE                        Date of sale\n\tSALES_ID                    Unique sale ID \n\t\t\t\t","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194743_626940359","id":"20161116-145110_1836663605","result":{"code":"SUCCESS","type":"HTML","msg":"<p>For your convenience, the metadata for the sample data files is listed out here again.</p>\n<p>Our sample data is from a fictitious computer retailer. The company sells computer parts and generally serves a single State in the country.</p>\n<h3>Customer.csv:</h3>\n<pre><code>Purpose: Hold customer records.\n\nColumns: \nFNAME                       Customer’s First Name\nLNAME                       Customer’s Last Name\nSTATUS                      Active or Inactive status\nTELNO                       Telephone # Customer’s unique ID\nCUSTOMER_ID CITY|ZIP        City and Zip code separated by the “|” character.\n</code></pre>\n<h3>Product.csv:</h3>\n<pre><code>Purpose: Hold product records.\n\nColumns:\nPROD_NAME                   Name of product\nDESCRIPTION                 Description of computer product\nCATEGORY                    Category product belongs to \nQTY_ON_HAND                 Quantity of product in warehouse\nPROD_NUM                    Unique product number   \nPACKAGED_WITH               Colon separated list of things that come in package with product.\n</code></pre>\n<h3>Sales.csv:</h3>\n<pre><code>Purpose: Holds all historical sales records. Company updates once a month. \n\nColumns:\nCUST_ID                     ID of customer who made purchase\nPROD_NUM                    ID of product that was purchased\nQTY                         QTY purchased\nDATE                        Date of sale\nSALES_ID                    Unique sale ID\n</code></pre>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6538"},{"title":"Loading Data","text":"%md\nIn the previous lab, you created 4 new tables. These tables need to have data in them to be useful. The table names are customer, products, sales_staging, and sales. Remember that the customer table is an External table and you already “loaded” data into it.\n\nLet’s see what the /user/hive/warehouse/computersalesdb.db directory currently contains on HDFS. This is the path of the directory that all of our Hive managed tables will be stored in.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194743_626940359","id":"20161116-145142_360935529","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In the previous lab, you created 4 new tables. These tables need to have data in them to be useful. The table names are customer, products, sales_staging, and sales. Remember that the customer table is an External table and you already “loaded” data into it.</p>\n<p>Let’s see what the /user/hive/warehouse/computersalesdb.db directory currently contains on HDFS. This is the path of the directory that all of our Hive managed tables will be stored in.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6539"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive/warehouse/computersalesdb.db\"'","dateUpdated":"2017-05-11T09:24:47+0000","config":{"colWidth":12,"editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194743_626940359","id":"20161116-145207_552603342","result":{"code":"ERROR","type":"TEXT","msg":"stdin: is not a tty\n17/05/11 21:24:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nls: `/user/hive/warehouse/computersalesdb.db': No such file or directory\nstdin: is not a tty\n17/05/11 21:24:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nls: `/user/hive/warehouse/computersalesdb.db': No such file or directory\nExitValue: 1"},"dateCreated":"2017-05-08T02:19:54+0000","dateStarted":"2017-05-11T09:24:47+0000","dateFinished":"2017-05-11T09:24:51+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:6540"},{"text":"%md\nNow you will copy the rest of the data files to a location on HDFS. First let’s make a new directory to put the data files in. Let’s create this /tmp/tempdata directory on HDFS. \n\nYou will then load the tables with this data in the next section of the lab.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194743_626940359","id":"20161116-145216_776258178","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now you will copy the rest of the data files to a location on HDFS. First let’s make a new directory to put the data files in. Let’s create this /tmp/tempdata directory on HDFS.</p>\n<p>You will then load the tables with this data in the next section of the lab.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6541"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -mkdir /tmp/tempdata\"'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194744_625016614","id":"20161116-145237_1348897632","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6542"},{"text":"%md\nUse the <b>chmod</b> command to set the permission for /tmp/tempdata to 777 (full read/write permission).","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194744_625016614","id":"20161116-145246_200983693","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Use the <b>chmod</b> command to set the permission for /tmp/tempdata to 777 (full read/write permission).</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6543"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -chmod 777 /tmp/tempdata\"'\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /tmp\"'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194744_625016614","id":"20161116-145255_1211124249","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6544"},{"text":"%md\nNext, let’s just copy our whole local /tmp/shared_hive_data directory on the hive container, into the /tmp/tempdata directory that you just created on HDFS.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194744_625016614","id":"20161116-145344_2062234088","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Next, let’s just copy our whole local /tmp/shared_hive_data directory on the hive container, into the /tmp/tempdata directory that you just created on HDFS.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6545"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -put /tmp/hive_data/ /tmp/tempdata\"'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194744_625016614","id":"20161116-145606_78973836","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6546"},{"text":"%md\nList out the contents of the /tmp/tempdata directory on HDFS.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194745_624631866","id":"20161116-145718_1144893060","result":{"code":"SUCCESS","type":"HTML","msg":"<p>List out the contents of the /tmp/tempdata directory on HDFS.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6547"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /tmp/tempdata\"'\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /tmp/tempdata/hive_data\"'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194745_624631866","id":"20161116-145747_1683674879","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6548"},{"title":"Loading Data into the Managed Non-Partitioned Tables","text":"%md\nThe first table you created in the previous lab was the products table. This table is fully managed by Hive and does not contain any partitions. We will now load the products table with the data that is stored in the local /tmp/tempdata/hive_data/Product.csv file on HDFS.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194745_624631866","id":"20161116-145808_1886634585","result":{"code":"SUCCESS","type":"HTML","msg":"<p>The first table you created in the previous lab was the products table. This table is fully managed by Hive and does not contain any partitions. We will now load the products table with the data that is stored in the local /tmp/tempdata/hive_data/Product.csv file on HDFS.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6549"},{"text":"%jdbc(hive)\nLOAD DATA INPATH '/tmp/tempdata/hive_data/Product.csv' OVERWRITE INTO TABLE computersalesdb.products","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194745_624631866","id":"20161116-145926_2092398325","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6550"},{"text":"%md\nHive copies the data from the file. Our products table now has data in it. Let’s check it out!\n\nList out the contents of the /user/hive/warehouse/computersalesdb.db/products directory. You will see the Product.csv file is there within that directory.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194745_624631866","id":"20161116-152812_1712627187","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Hive copies the data from the file. Our products table now has data in it. Let’s check it out!</p>\n<p>List out the contents of the /user/hive/warehouse/computersalesdb.db/products directory. You will see the Product.csv file is there within that directory.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6551"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive/warehouse/computersalesdb.db/products\"'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194746_625786112","id":"20161116-152900_1964488849","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6552"},{"text":"%md\nYou can easily view the contents of the Product.csv file by running the <b>hdfs dfs -cat</b> command.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194746_625786112","id":"20161116-152930_1148576255","result":{"code":"SUCCESS","type":"HTML","msg":"<p>You can easily view the contents of the Product.csv file by running the <b>hdfs dfs -cat</b> command.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6553"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -cat /user/hive/warehouse/computersalesdb.db/products/Product.csv\"'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194746_625786112","id":"20161116-153149_621301466","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6554"},{"text":"%md\nLook at the data displayed - our products table is now loaded into Hive!\n\nNext you will load sales records into the sales_staging table. ","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194746_625786112","id":"20161116-153226_786410413","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Look at the data displayed - our products table is now loaded into Hive!</p>\n<p>Next you will load sales records into the sales_staging table.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6555"},{"text":"%jdbc(hive)\nLOAD DATA INPATH '/tmp/tempdata/hive_data/Sales.csv' INTO TABLE computersalesdb.sales_staging\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194746_625786112","id":"20161116-153423_2123055124","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6556"},{"text":"%md\nNotice you left off the OVERWRITE keyword in this statement. This is because we normally would want to add the monthly sales data to our historical list of records already in the sales_staging table. \nWe would not want to overwrite all the records in that table with just this month’s data.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194747_625401363","id":"20161116-154214_156365266","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Notice you left off the OVERWRITE keyword in this statement. This is because we normally would want to add the monthly sales data to our historical list of records already in the sales_staging table.\n<br  />We would not want to overwrite all the records in that table with just this month’s data.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6557"},{"text":"%md\nAgain, let’s verify that the data is now within the Hive warehouse on HDFS, by checking out the file using the hdfs dfs –cat command.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194747_625401363","id":"20161116-154258_361559052","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Again, let’s verify that the data is now within the Hive warehouse on HDFS, by checking out the file using the hdfs dfs –cat command.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6558"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -cat /user/hive/warehouse/computersalesdb.db/sales_staging/Sales.csv\"'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194747_625401363","id":"20161116-165227_1795202087","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6559"},{"title":"Loading Data into the Managed Partitioned Table","text":"%md\nNow that the sales_staging table has data you can work with, let’s write some queries that will allow you to load the partitioned sales table (partitioned on sales_date) with data coming from sales_staging.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194748_623477619","id":"20161116-165321_436134454","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now that the sales_staging table has data you can work with, let’s write some queries that will allow you to load the partitioned sales table (partitioned on sales_date) with data coming from sales_staging.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6560"},{"text":"%md\nLoad sales data from ‘2012-01-09’ into a partition of the sales table.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194748_623477619","id":"20161116-165417_968747058","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Load sales data from ‘2012-01-09’ into a partition of the sales table.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6561"},{"text":"%jdbc(hive)\nINSERT OVERWRITE TABLE computersalesdb.sales\n\t PARTITION (sales_date = '2012-01-09')\n\t SELECT cust_id, prod_num, qty, sales_id\n            FROM computersalesdb.sales_staging ss\n            WHERE ss.sale_date = '2012-01-09'\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194748_623477619","id":"20161116-165438_1503568173","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6562"},{"text":"%md\n\nKeep in mind as you run these queries, that some of them are being handled by Hive in \"local\" mode - specifically the queries that don't require MapReduce, while others, most likely the more complex ones in this lab wil trigger MapReduce jobs to run on Hadoop.\n\nNext, do a hdfs dfs –ls command and take a look in the sales folder now. You will see a new subdirectory named sales_date=2012-01-09. Within that directory is a data file named 000000_0. If you cat that file you will see that it only contains the data for our 2012-01-09 sales.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194748_623477619","id":"20161116-165537_491103727","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Keep in mind as you run these queries, that some of them are being handled by Hive in &ldquo;local&rdquo; mode - specifically the queries that don't require MapReduce, while others, most likely the more complex ones in this lab wil trigger MapReduce jobs to run on Hadoop.</p>\n<p>Next, do a hdfs dfs –ls command and take a look in the sales folder now. You will see a new subdirectory named sales_date=2012-01-09. Within that directory is a data file named 000000_0. If you cat that file you will see that it only contains the data for our 2012-01-09 sales.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6563"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive/warehouse/computersalesdb.db/sales\"'\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive/warehouse/computersalesdb.db/sales/sales_date=2012-01-09\"'\nssh  root@hive 'su - root bash -c \"hdfs dfs -cat /user/hive/warehouse/computersalesdb.db/sales/sales_date=2012-01-09/000000_0\"'\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194748_623477619","id":"20161116-165656_1843628086","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6564"},{"text":"%md\nFollowing the same procedures, load sales data from ‘2012-01-24’ into a new partition of the sales table.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194749_623092870","id":"20161116-165908_2102989876","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Following the same procedures, load sales data from ‘2012-01-24’ into a new partition of the sales table.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6565"},{"text":"%jdbc(hive)\nINSERT OVERWRITE TABLE computersalesdb.sales\n\t PARTITION (sales_date = '2012-01-24')\n\t SELECT cust_id, prod_num, qty, sales_id\n            FROM computersalesdb.sales_staging ss\n            WHERE ss.sale_date = '2012-01-24'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194749_623092870","id":"20161116-170016_1078422609","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6566"},{"text":"%md\nRun the hdfs dfs -ls command and take a look in the sales folder now. You will see a new subdirectory named sales_date=2012-01-24. Within that directory is a data file named 000000_0. \nIf you cat that file you will see that it only contains the data for our 2012-01-24 sales.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194749_623092870","id":"20161116-170045_33174310","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Run the hdfs dfs -ls command and take a look in the sales folder now. You will see a new subdirectory named sales_date=2012-01-24. Within that directory is a data file named 000000_0.\n<br  />If you cat that file you will see that it only contains the data for our 2012-01-24 sales.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6567"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive/warehouse/computersalesdb.db/sales\"'\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /user/hive/warehouse/computersalesdb.db/sales/sales_date=2012-01-24\"'\nssh  root@hive 'su - root bash -c \"hdfs dfs -cat /user/hive/warehouse/computersalesdb.db/sales/sales_date=2012-01-24/000000_0\"'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194749_623092870","id":"20161116-170123_365646386","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6568"},{"title":"Running Queries","text":"","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194749_623092870","id":"20161116-170200_1047683422","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6569"},{"title":"Selecting Data","text":"%md\nNow that the tables have data in them, let’s start running queries against that data.\n\nSelect all the data in the products table where the product category is ‘Video’. Order the results by prod_num.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194750_624247117","id":"20161116-170238_1756633824","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now that the tables have data in them, let’s start running queries against that data.</p>\n<p>Select all the data in the products table where the product category is ‘Video’. Order the results by prod_num.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6570"},{"text":"%jdbc(hive)\nSELECT * FROM computersalesdb.products WHERE category= 'Video' ORDER BY prod_num","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194750_624247117","id":"20161116-170311_1639182801","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6571"},{"text":"%md\nNow select all the products where category=’Video’ AND the first element of the PACKAGED_WITH array contains ‘dvd’.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194750_624247117","id":"20161116-170358_1808426429","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now select all the products where category=’Video’ AND the first element of the PACKAGED_WITH array contains ‘dvd’.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6572"},{"text":"%jdbc(hive)\nSELECT * FROM computersalesdb.products WHERE category= 'Video' AND PACKAGED_WITH[0]= 'dvd'\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194750_624247117","id":"20161116-170516_1233075788","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6573"},{"text":"%md\nFind out how many products there are for each category of item by using the GROUP BY clause.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194750_624247117","id":"20161116-170544_1357447617","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Find out how many products there are for each category of item by using the GROUP BY clause.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6574"},{"text":"%jdbc(hive)\nSELECT category, count(*) FROM computersalesdb.products GROUP BY category\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194751_623862368","id":"20161116-170624_1168578328","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6575"},{"text":"%md\nUse a nested select to show the product categories that contain more than 3 products.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194751_623862368","id":"20161116-170652_80271229","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Use a nested select to show the product categories that contain more than 3 products.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6576"},{"text":"%jdbc(hive)\nFROM(\n    SELECT category, count(*) as count FROM computersalesdb.products \n    GROUP BY category) cats\nSELECT * WHERE cats.count > 3\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194751_623862368","id":"20161116-170719_1580549438","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6577"},{"text":"%md\nYour output should like similar to the above screen capture. You will notice that in addition to using a subquery you also used two column aliases (“count” and “cats”).","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194751_623862368","id":"20161116-170804_1615507861","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Your output should like similar to the above screen capture. You will notice that in addition to using a subquery you also used two column aliases (“count” and “cats”).</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6578"},{"title":"Taking Advantage of Partitioned Data","text":"%md\nThe sales table is partitioned on sales_date. In a previous exercise you loaded data into two partitions of this table (2012-01-09 and 2012-01-24 partitions). Let’s take advantage of this partitioning to improve latency.\n\nRun a SELECT query that finds only the sales that occurred on 2012-01-24. Order the results by the sale_id.\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194751_623862368","id":"20161116-170821_1584947858","result":{"code":"SUCCESS","type":"HTML","msg":"<p>The sales table is partitioned on sales_date. In a previous exercise you loaded data into two partitions of this table (2012-01-09 and 2012-01-24 partitions). Let’s take advantage of this partitioning to improve latency.</p>\n<p>Run a SELECT query that finds only the sales that occurred on 2012-01-24. Order the results by the sale_id.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6579"},{"text":"%jdbc(hive)\nSELECT * FROM computersalesdb.sales\nWHERE sales_date = '2012-01-24'\nORDER BY sales_id\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194752_314139503","id":"20161116-170930_1111807602","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6580"},{"text":"%md\nAbove, you will see that just the 2012-01-24 sales records were returned.\n\nOnly the sales_id=2012-01-24 partition file had to be read in to complete this query, saving you some wait time – theoretically a lot of wait time if you had a large number of historical sales data.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194752_314139503","id":"20161116-171013_199465213","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Above, you will see that just the 2012-01-24 sales records were returned.</p>\n<p>Only the sales_id=2012-01-24 partition file had to be read in to complete this query, saving you some wait time – theoretically a lot of wait time if you had a large number of historical sales data.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6581"},{"title":"Joins","text":"%md\nLet’s show all Optical category sales that occurred on 2012-01-09. You will need to do an equi-join between the sales and products tables to gather this information.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194752_314139503","id":"20161116-171405_707768456","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Let’s show all Optical category sales that occurred on 2012-01-09. You will need to do an equi-join between the sales and products tables to gather this information.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6582"},{"text":"%jdbc(hive)\nSELECT s.cust_id, s.prod_num, s.qty, s.sales_id, \n      p.prod_name, p.category\n      FROM computersalesdb.sales s JOIN computersalesdb.products p\n           ON s.prod_num = p.prod_num\n      WHERE s.sales_date = '2012-01-09' AND p.category = 'Optical'\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194752_314139503","id":"20161116-175122_2024347250","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6583"},{"text":"%md\nThe join was successful. You got the 3 records we were looking for.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194752_314139503","id":"20161116-175156_2113652617","result":{"code":"SUCCESS","type":"HTML","msg":"<p>The join was successful. You got the 3 records we were looking for.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6584"},{"title":"Views","text":"%md\nNow let’s create a View that stores a query which returns all the sales records (joined with products table) where the product category is ‘Optical’.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194753_313754754","id":"20161116-175234_970878130","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now let’s create a View that stores a query which returns all the sales records (joined with products table) where the product category is ‘Optical’.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6585"},{"text":"%jdbc(hive)\nCREATE VIEW optical_sales AS\n        SELECT s.cust_id, s.prod_num, s.qty, s.sales_id, \n        p.prod_name, p.category\n        FROM computersalesdb.sales s JOIN computersalesdb.products p\n             ON s.prod_num = p.prod_num\n        WHERE p.category = 'Optical'\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194753_313754754","id":"20161116-175305_1962496956","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6586"},{"text":"%md\nThe View was successfully created.\n\nNow that you have the optical_sales view, you can use it within other queries, just like it were a table. Notice how short this query is.\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194753_313754754","id":"20161116-175337_1041646530","result":{"code":"SUCCESS","type":"HTML","msg":"<p>The View was successfully created.</p>\n<p>Now that you have the optical_sales view, you can use it within other queries, just like it were a table. Notice how short this query is.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6587"},{"text":"%jdbc(hive)\nSELECT * FROM optical_sales\nWHERE  qty > 1\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194753_313754754","id":"20161116-175410_409835805","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6588"},{"title":"Exporting Data","text":"%md\nImagine your management team wants you to extract all sales of Optical devices and give it to them in a format outside of the Hive environment. You could do this by exporting the data from Hive.\n\nYou are going to create a new directory on HDFS called “reports”. Create this directory within the /tmp directory on HDFS.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194753_313754754","id":"20161116-175439_706429097","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Imagine your management team wants you to extract all sales of Optical devices and give it to them in a format outside of the Hive environment. You could do this by exporting the data from Hive.</p>\n<p>You are going to create a new directory on HDFS called “reports”. Create this directory within the /tmp directory on HDFS.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6589"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -mkdir /tmp/reports\"'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194753_313754754","id":"20161116-175641_1009033519","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6590"},{"text":"%md\nMake this new folder writable by all users.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194754_314909001","id":"20161116-175707_1980001142","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Make this new folder writable by all users.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6591"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -chmod 777 /tmp/reports\"'\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /tmp\"'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194754_314909001","id":"20161116-175727_624408926","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6592"},{"text":"%md\nNow utilize the previous query and write the data to the /tmp/reports directory on the HDFS file system.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194754_314909001","id":"20161116-175751_1546879792","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now utilize the previous query and write the data to the /tmp/reports directory on the HDFS file system.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6593"},{"text":"%jdbc(hive)\nINSERT OVERWRITE DIRECTORY '/tmp/reports'\nSELECT * FROM optical_sales WHERE  qty > 1","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194754_314909001","id":"20161116-175826_172553201","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6594"},{"text":"%md\nNow check the HDFS /tmp/reports directory.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194754_314909001","id":"20161116-180721_1990704609","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now check the HDFS /tmp/reports directory.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6595"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -ls /tmp/reports\"'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194755_314524252","id":"20161116-180754_509310788","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6596"},{"text":"%md\nNotice there is one new file called 000000_0! This is the data that Hive wrote out.\n\nNow cat this file and see what the contents look like.\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194755_314524252","id":"20161116-180816_2019506382","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Notice there is one new file called 000000_0! This is the data that Hive wrote out.</p>\n<p>Now cat this file and see what the contents look like.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6597"},{"text":"%sh\nssh  root@hive 'su - root bash -c \"hdfs dfs -cat /tmp/reports/000000_0\"'","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194755_314524252","id":"20161116-180843_1134829395","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6598"},{"text":"%md\nYou could copy the new report out of HDFS and work with the results in the tools of your choice.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194755_314524252","id":"20161116-181242_894231107","result":{"code":"SUCCESS","type":"HTML","msg":"<p>You could copy the new report out of HDFS and work with the results in the tools of your choice.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6599"},{"title":"Explain","text":"%md\nLet’s take a brief look at using EXPLAIN in a Hive query.\n\nYou will have Hive explain the execution plan for a simple query that selects all customer records.\n","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194755_314524252","id":"20161116-181307_637437953","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Let’s take a brief look at using EXPLAIN in a Hive query.</p>\n<p>You will have Hive explain the execution plan for a simple query that selects all customer records.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6600"},{"text":"%jdbc(hive)\nEXPLAIN SELECT * FROM computersalesdb.customer","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194756_312600508","id":"20161116-181405_999698883","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6601"},{"text":"%md\nCan you tell how many MapReduce jobs would be required to run this Hive query? If you guessed none, you would be correct. \nHive is able to read the records and dump the output to the console without using MapReduce. Hive is using “local mode” to do this.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194756_312600508","id":"20161116-181439_978926329","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Can you tell how many MapReduce jobs would be required to run this Hive query? If you guessed none, you would be correct.\n<br  />Hive is able to read the records and dump the output to the console without using MapReduce. Hive is using “local mode” to do this.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6602"},{"text":"%md\n\nCongratulations! You now know how to load data into Hive. You are able to run a variety of queries using familiar clauses such as SELECT, WHERE, GROUP BY, ORDER BY, JOIN, and more.\nYou can export data out of Hive and even run the Explain tool to get an execution plan for your query. You may move on to the next Unit.","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194756_312600508","id":"20161116-181640_703507043","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Congratulations! You now know how to load data into Hive. You are able to run a variety of queries using familiar clauses such as SELECT, WHERE, GROUP BY, ORDER BY, JOIN, and more.\n<br  />You can export data out of Hive and even run the Explain tool to get an execution plan for your query. You may move on to the next Unit.</p>\n"},"dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6603"},{"text":"","dateUpdated":"2017-05-08T02:19:54+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1494253194756_312600508","id":"20161116-181717_855398876","dateCreated":"2017-05-08T02:19:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6604"}],"name":"Hive Lab 3","id":"2CGNFUZTC","angularObjects":{"2BZJ3HWCJ:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}